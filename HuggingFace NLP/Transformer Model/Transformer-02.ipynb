{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM91psd+aik/suM9JwbyKK2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"7G4krHSpUxGe","executionInfo":{"status":"ok","timestamp":1729183758175,"user_tz":-345,"elapsed":2564,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"outputs":[],"source":["## creating a transformer\n","\n","# only model (not trained model)\n","from transformers import BertConfig, BertModel\n","\n","# building the config\n","config = BertConfig()\n","\n","# building the model from the config above\n","model = BertModel(config)"]},{"cell_type":"code","source":["# view the config\n","print(\"Bert configuration\")\n","print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouGP7OChX5xe","executionInfo":{"status":"ok","timestamp":1729183759419,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"f27299b2-ceb5-42cd-d8c5-9dbbc1ed6ee4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Bert configuration\n","BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.44.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","source":["from transformers import BertModel\n","\n","model = BertModel.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"rqfbg6TbYZ-H","executionInfo":{"status":"ok","timestamp":1729183815916,"user_tz":-345,"elapsed":477,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09jh02DMYyKa","executionInfo":{"status":"ok","timestamp":1729183821001,"user_tz":-345,"elapsed":488,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"7f31a232-445f-49da-d810-8b4045c8ec54"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSdpaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["The model above `bert-base-cased` is model checkpoint and already trained on large dataset and now we can use this for different purpose and also can fine tune on our own dataset."],"metadata":{"id":"E44WiMDtY7-V"}},{"cell_type":"code","source":["### saving the model\n","model.save_pretrained(\"./bert\")"],"metadata":{"id":"L-gHJFdoZL4M","executionInfo":{"status":"ok","timestamp":1729184002601,"user_tz":-345,"elapsed":6835,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!ls ./bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zpair_wmZvGW","executionInfo":{"status":"ok","timestamp":1729184077185,"user_tz":-345,"elapsed":507,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"14a613d0-c092-494e-f763-a35eea675302"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["config.json  model.safetensors\n"]}]},{"cell_type":"code","source":["### inference\n","\n","# example text\n","sequences = [\"Hello\", \"Hi\", \"Hey\"]\n","\n","# example tokenized output\n","encoded_sequences = [\n","    [101, 7592, 102],\n","    [101, 7592, 102],\n","    [101, 7592, 102]\n","]"],"metadata":{"id":"O1fu0zTAZ4sG","executionInfo":{"status":"ok","timestamp":1729184256195,"user_tz":-345,"elapsed":450,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)"],"metadata":{"id":"8DOGPhDsadLY","executionInfo":{"status":"ok","timestamp":1729184282213,"user_tz":-345,"elapsed":479,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["output = model(model_inputs)"],"metadata":{"id":"GpvTLPOVaFxY","executionInfo":{"status":"ok","timestamp":1729184294372,"user_tz":-345,"elapsed":1218,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjRiJ4hZar8M","executionInfo":{"status":"ok","timestamp":1729184321317,"user_tz":-345,"elapsed":481,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"b9eee801-4c11-400b-fe8c-05de861ffff1"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1346,  0.2425,  0.0835,  ...,  0.0392,  0.1245, -0.2297],\n","         [-0.1441, -0.3777,  0.3432,  ..., -0.2696,  0.5613, -0.1820],\n","         [ 1.3870,  0.2144, -0.5577,  ...,  0.4530,  0.4656,  0.2042]],\n","\n","        [[ 0.1346,  0.2425,  0.0835,  ...,  0.0392,  0.1245, -0.2297],\n","         [-0.1441, -0.3777,  0.3432,  ..., -0.2696,  0.5613, -0.1820],\n","         [ 1.3870,  0.2144, -0.5577,  ...,  0.4530,  0.4656,  0.2042]],\n","\n","        [[ 0.1346,  0.2425,  0.0835,  ...,  0.0392,  0.1245, -0.2297],\n","         [-0.1441, -0.3777,  0.3432,  ..., -0.2696,  0.5613, -0.1820],\n","         [ 1.3870,  0.2144, -0.5577,  ...,  0.4530,  0.4656,  0.2042]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2460,  0.2466,  0.9985,  ...,  0.9997, -0.8470,  0.9821],\n","        [-0.2460,  0.2466,  0.9985,  ...,  0.9997, -0.8470,  0.9821],\n","        [-0.2460,  0.2466,  0.9985,  ...,  0.9997, -0.8470,  0.9821]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Tokenizer\n","Main purpose is to convert text into numerical.\n","\n","## Some of tokenization algorithm\n","- `**Word-based**`: split on spaces\n","- `**Character-based**`: split into character\n","* `**Subword**`: frequentlu used words should not be split into smaller subwords but rare used words should be decomposed into meaningful subwords.\n","- some other are `Byte-level BPE`, `WordPiece`, `SentencePiece`."],"metadata":{"id":"H_7pTu8Lbmro"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"oJmpfmXTcy4e","executionInfo":{"status":"ok","timestamp":1729185315282,"user_tz":-345,"elapsed":479,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") # automatically get the tokenizer for the model"],"metadata":{"id":"epsjlXWVehsF","executionInfo":{"status":"ok","timestamp":1729185366603,"user_tz":-345,"elapsed":526,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["tokenizer(\"Hello this is really beautiful\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MT5pftcesxa","executionInfo":{"status":"ok","timestamp":1729185385833,"user_tz":-345,"elapsed":643,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"d221fad0-5add-4444-911f-9cd705c08662"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 8667, 1142, 1110, 1541, 2712, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["### saving tokenizer\n","tokenizer.save_pretrained(\"./tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YAiIe4o0ezEa","executionInfo":{"status":"ok","timestamp":1729185408448,"user_tz":-345,"elapsed":666,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"9846779b-a886-44f4-8f6c-24e7070b828b"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./tokenizer/tokenizer_config.json',\n"," './tokenizer/special_tokens_map.json',\n"," './tokenizer/vocab.txt',\n"," './tokenizer/added_tokens.json',\n"," './tokenizer/tokenizer.json')"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## Tokenization\n","-  text -> split -> add [CLS] at the start and [SEP] at the end with 101 and 102 token value and then tokenize the split text.\n","- `**Encoding**`: tokenization followed by the conversion to input id so first tokenize the text and then convert into number using `vocabulary`.\n"],"metadata":{"id":"fV4o3Hi_e2A7"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = \"Hello this is really beautiful\"\n","\n","tokens = tokenizer.tokenize(sequence) # tokenize()\n","\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frfF25twgbNL","executionInfo":{"status":"ok","timestamp":1729185937881,"user_tz":-345,"elapsed":480,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"09968105-2f2c-4f54-ed45-d7da15818f08"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'this', 'is', 'really', 'beautiful']\n"]}]},{"cell_type":"code","source":["tokenize_nepali = tokenizer.tokenize(\"nepal sundar desh ho.\")"],"metadata":{"id":"JG30L6F4f6zc","executionInfo":{"status":"ok","timestamp":1729186005657,"user_tz":-345,"elapsed":557,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["print(tokenize_nepali) # subword tokenizer split word until it obtains token that can be represented by its vocabulary."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKsjLDQShH-d","executionInfo":{"status":"ok","timestamp":1729186132674,"user_tz":-345,"elapsed":487,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"8d1a994c-5682-4c28-dd6b-8257ba20256c"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["['ne', '##pal', 'sun', '##dar', 'des', '##h', 'ho', '.']\n"]}]},{"cell_type":"code","source":["tokenize_another = tokenizer.tokenize(\"Environmental education should be given to everyone.\")"],"metadata":{"id":"m-0AmGuuhHVl","executionInfo":{"status":"ok","timestamp":1729186135101,"user_tz":-345,"elapsed":451,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["tokenize_another"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofM9yGdVf51E","executionInfo":{"status":"ok","timestamp":1729186135599,"user_tz":-345,"elapsed":1,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"5fb683b1-e71c-457b-d515-ae36ed23e5a6"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Environmental', 'education', 'should', 'be', 'given', 'to', 'everyone', '.']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["tokenize = tokenizer.tokenize(\"Learning by doing.\")\n","tokenize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d534IZd3hCGz","executionInfo":{"status":"ok","timestamp":1729186166490,"user_tz":-345,"elapsed":463,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"9242fd2c-c7e9-4d71-b08c-942747ae9e26"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Learning', 'by', 'doing', '.']"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["### token to number\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(tokens), print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZM_W4Mjhvj6","executionInfo":{"status":"ok","timestamp":1729186225051,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"c2fa6272-af40-490f-a033-9796beff8ec6"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'this', 'is', 'really', 'beautiful']\n","[8667, 1142, 1110, 1541, 2712]\n"]},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["## Decoding"],"metadata":{"id":"hqsGykfriLi-"}},{"cell_type":"code","source":["decoded_string = tokenizer.decode([8667, 1142, 1110, 1541, 2712])\n","print(f\"Decoded text: {decoded_string}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vUl-E-9iObP","executionInfo":{"status":"ok","timestamp":1729186378519,"user_tz":-345,"elapsed":457,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"2d03a480-05da-4b36-c63e-5033943fc29d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded text: Hello this is really beautiful\n"]}]},{"cell_type":"markdown","source":["### Handling multiple sentence"],"metadata":{"id":"TVZgIx5AidTT"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sentence = \"I am happy\"\n","\n","tokens = tokenizer.tokenize(sentence)\n","token_id = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = torch.tensor([token_id])\n","print(input_ids)\n","# here add extra dimension because it expect multiple sentences by default.\n","\n","output = model(input_ids)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"US9Au4d7jE_B","executionInfo":{"status":"ok","timestamp":1729187398292,"user_tz":-345,"elapsed":476,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"73288c03-21b7-47ec-f41d-e928714657ac"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1045, 2572, 3407]])\n","SequenceClassifierOutput(loss=None, logits=tensor([[-3.4687,  3.6865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"]}]},{"cell_type":"code","source":["## batching is the act of sending multiple sentences, but when we have one we can just build a batch with it.\n","\n","batched_ids = [token_id, token_id]"],"metadata":{"id":"nzelyDedmcKw","executionInfo":{"status":"ok","timestamp":1729187460774,"user_tz":-345,"elapsed":475,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["batched_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekEv0txXmyyi","executionInfo":{"status":"ok","timestamp":1729187495648,"user_tz":-345,"elapsed":461,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"eb6b3929-e212-4ce2-e2dc-b865998d749a"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1045, 2572, 3407], [1045, 2572, 3407]]"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["## batching two sentence of variable length\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200]\n","]\n","\n","# for this we use padding technique\n","# padding make sure all our sentence have the same length by adding a special word called padding token.\n","# if we have 20 sentence with 20 words and 1 sentence with 10 word, padding will ensure that all the sentences have equal words and use padding to make sure each have same size.\n","\n","padding_id = 100\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, padding_id]\n","]"],"metadata":{"id":"bMXIcIiOiXjb","executionInfo":{"status":"ok","timestamp":1729187657049,"user_tz":-345,"elapsed":443,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"],"metadata":{"id":"ZW1nPzikndcX","executionInfo":{"status":"ok","timestamp":1729187766239,"user_tz":-345,"elapsed":482,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]"],"metadata":{"id":"5g1eR-k-n2SQ","executionInfo":{"status":"ok","timestamp":1729187786400,"user_tz":-345,"elapsed":718,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id]\n","]"],"metadata":{"id":"HMmPIrZzn60e","executionInfo":{"status":"ok","timestamp":1729187825959,"user_tz":-345,"elapsed":449,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)\n","# logit for different sequence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VgKoIrOnc-U","executionInfo":{"status":"ok","timestamp":1729187922891,"user_tz":-345,"elapsed":463,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"c12581c1-d2b3-49e7-874b-60057501dfbf"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n","tensor([[ 1.5694, -1.3895],\n","        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["### Attention mask:\n","# to get the same result when the same token are there but one is with padding we can tell to not consider the padding.\n","# 0: donot consider the padding ignore token\n","# 1: consider the padding also\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id]\n","]\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0]\n","]\n","output = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKAz4qpnortZ","executionInfo":{"status":"ok","timestamp":1729188188587,"user_tz":-345,"elapsed":446,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"e0aab015-2059-498b-8a6e-bc7ea0292fd5"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895],\n","        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"]}]},{"cell_type":"markdown","source":["### Longer sequences"],"metadata":{"id":"kIYZ4jYMpiwh"}},{"cell_type":"code","source":["## there is a limit on how long sentence we can take at once.\n","## model hacve different supporting.\n","## truncate specifying the max length\n","\n","sentence = \"This is a really long sentence\"\n","max_length = 10\n","\n","truncate = sentence[:max_length]\n","print(f\"Original: {sentence}\"), print(f\"After: {truncate}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtGxjRj3pkjI","executionInfo":{"status":"ok","timestamp":1729188399791,"user_tz":-345,"elapsed":454,"user":{"displayName":"Manoj Baniya","userId":"01713573044371624098"}},"outputId":"d1e6b5bc-49be-4eb6-c5d9-0b0ece822b69"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Original: This is a really long sentence\n","After: This is a \n"]},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{},"execution_count":59}]}]}