{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-2-7b quantized model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Generative AI\\Gen AI Language\\pdfbot\\bot\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pinecone setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdf\n",
    "def load_pdf(data_dir):\n",
    "    loader = DirectoryLoader(path = data_dir,\n",
    "                             glob=\"*.pdf\",\n",
    "                             loader_cls = PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(extracted_data[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 20\n",
    "    )\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    \n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_split(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_embedding_model():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Generative AI\\Gen AI Language\\pdfbot\\bot\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = download_embedding_model()\n",
    "# this embedding model has 384 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embedding.embed_query(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.08469949662685394,\n",
       " 0.4564874470233917,\n",
       " 0.10086802393198013,\n",
       " -0.1100568175315857,\n",
       " 0.04533900320529938,\n",
       " -0.3661496043205261,\n",
       " 0.35803312063217163,\n",
       " -0.07830098271369934,\n",
       " -0.24361982941627502,\n",
       " 0.2736356854438782,\n",
       " 0.10803009569644928,\n",
       " -0.7733527421951294,\n",
       " -0.1324445903301239,\n",
       " 0.19357401132583618,\n",
       " 0.2035713493824005,\n",
       " -0.3699710965156555,\n",
       " 0.33961641788482666,\n",
       " -0.6763171553611755,\n",
       " -0.849643886089325,\n",
       " -0.19371701776981354]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the PineCone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "index_name = \"pdfbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to the database\n",
    "\n",
    "# vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "#         text_chunks,\n",
    "#         index_name=index_name,\n",
    "#         embedding=embedding\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search\n",
    "query = \"what is self attention?\"\n",
    "vectorstore = PineconeVectorStore(index_name=index_name,embedding=embedding)\n",
    "\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].', metadata={'page': 1.0, 'source': 'data\\\\attention is all you need paper.pdf'}), Document(page_content='just\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'page': 13.0, 'source': 'data\\\\attention is all you need paper.pdf'}), Document(page_content='just\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'page': 14.0, 'source': 'data\\\\attention is all you need paper.pdf'}), Document(page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.', metadata={'page': 5.0, 'source': 'data\\\\attention is all you need paper.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following piece of information to answer the question. If you are not sure dont make up the answer just say i am not sure about it.\n",
    "\n",
    "context: {context}\n",
    "question: {question}\n",
    "\n",
    "only return the helpful answer.\n",
    "helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: unable to load local llm here using CTransformer\n",
    "\n",
    "llm = CTransformers(\n",
    "    model = \"llama-2-7b-chat.ggmlv3.q2_K.bin\",\n",
    "    model_type = \"llama\",\n",
    "    max_new_tokens = 200,\n",
    "    temperature = 0.7,\n",
    "    # n_gqa = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = LLMChain(\n",
    "#     prompt = prompt,\n",
    "#     llm = llm\n",
    "# )\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\n",
    "    \"prompt\": PROMPT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs = {\n",
    "            \"k\": 2\n",
    "        }\n",
    "    ),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Generative AI\\Gen AI Language\\pdfbot\\bot\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "result = qa({ \"query\" : \"what is self attention?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention is an attention mechanism that allows a model to focus on different positions within a single sequence when computing a representation of it.\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
