{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY_NEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "\n",
    "client = Groq()\n",
    "MODEL = \"llama3-8b-8192\"\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(user_prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculator function to perform calcuation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\", # function name need to match to our actual function name\n",
    "                \"description\": \"Evaluate a mathematical expression\", # description of our function\n",
    "                \"parameters\": { # parameter to our function\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": { # parameter name\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        stream=False,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    print(f\"Initial Response Message: {response_message}\\n\")\n",
    "    tool_calls = response_message.tool_calls\n",
    "    \n",
    "    if tool_calls:\n",
    "        print(\"Tool call is available...\\n\")\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate\n",
    "        }\n",
    "        \n",
    "        messages.append(response_message)\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            print(f\"Tool call: {tool_call}\\n\")\n",
    "            function_name = tool_call.function.name\n",
    "            print(f\"Using function {function_name} for {user_prompt}\")\n",
    "            function_to_call = available_functions.get(function_name)\n",
    "            function_arguments = json.loads(tool_call.function.arguments)\n",
    "            print(f\"Function argument: {function_arguments}\\n\")\n",
    "            \n",
    "            function_response = function_to_call(\n",
    "                expression=function_arguments.get(\"expression\")\n",
    "            )\n",
    "            print(f\"Got the response from the function {function_response}\")\n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response\n",
    "            })\n",
    "            \n",
    "        print(f\"Final message to the llm after function call : {messages}\")\n",
    "            \n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        return second_response.choices[0].message.content\n",
    "    else:\n",
    "        print(\"No tool call needed...\\n\")\n",
    "        return response_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response Message: ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_36v2', function=Function(arguments='{\"expression\":\"2 + 2\"}', name='calculate'), type='function')])\n",
      "\n",
      "Tool call is available...\n",
      "\n",
      "Tool call: ChatCompletionMessageToolCall(id='call_36v2', function=Function(arguments='{\"expression\":\"2 + 2\"}', name='calculate'), type='function')\n",
      "\n",
      "Using function calculate for what is 2 + 2?\n",
      "Function argument: {'expression': '2 + 2'}\n",
      "\n",
      "Got the response from the function {\"result\": 4}\n",
      "Final message to the llm after function call : [{'role': 'system', 'content': 'You are a calculator assistant. Use the calculator function to perform calcuation.'}, {'role': 'user', 'content': 'what is 2 + 2?'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_36v2', function=Function(arguments='{\"expression\":\"2 + 2\"}', name='calculate'), type='function')]), {'tool_call_id': 'call_36v2', 'role': 'tool', 'name': 'calculate', 'content': '{\"result\": 4}'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The result of the calculation is indeed 4.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conversation(\"what is 2 + 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response Message: ChatCompletionMessage(content=\"A banana is a type of long, curved fruit that grows in clusters on banana plants. It is typically yellow when it's ripe and has a sweet, creamy flavor. Bananas are a good source of several important nutrients, including potassium, vitamins C and B6, and fiber.\", role='assistant', function_call=None, tool_calls=None)\n",
      "\n",
      "No tool call needed...\n",
      "\n",
      "A banana is a type of long, curved fruit that grows in clusters on banana plants. It is typically yellow when it's ripe and has a sweet, creamy flavor. Bananas are a good source of several important nutrients, including potassium, vitamins C and B6, and fiber.\n"
     ]
    }
   ],
   "source": [
    "response = run_conversation(\"What is banana?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTING_MODEL = \"llama3-70b-8192\"\n",
    "TOOL_USE_MODEL = \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "GENERAL_MODEL = \"llama3-70b-8192\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(expression):\n",
    "    \"\"\"Tool to evaluate a mathematical expression\"\"\"\n",
    "    print(\"model is using calculate tool\")\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(query):\n",
    "    \"\"\"Routing logic to let LLM decide if tools are needed\"\"\"\n",
    "    routing_prompt = f\"\"\"\n",
    "    Given the following user query, determine if any tools are needed to answer it.\n",
    "    If a calculator tool is need, response with 'TOOL: CALCULATE'.\n",
    "    If no tools are needed, responsd with 'NO TOOL'\n",
    "    \n",
    "    User Query: {query}\n",
    "    \n",
    "    Response:\\n\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=ROUTING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are a routing assistant. Determine if tools are needed to answer the user query.\"},\n",
    "            {\"role\": \"user\", \"content\": routing_prompt}\n",
    "        ],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    \n",
    "    routing_decision = response.choices[0].message.content.strip()\n",
    "    \n",
    "    if \"TOOL: CALCULATE\" in routing_decision:\n",
    "        return \"calculate tool needed\"\n",
    "    else:\n",
    "        return \"no tool needed to answer the query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_tool(query):\n",
    "    \"\"\"Use the tool to perform the calculation\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculator function to perform calcuation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=TOOL_USE_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=3000\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    \n",
    "    if tool_calls:\n",
    "        print(\"Tool call use available...\\n\")\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            print(f\"tool call: {tool_call}\\n\")\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = calculate(function_args.get(\"expression\"))\n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": \"calculate\",\n",
    "                \"content\": function_response\n",
    "            })\n",
    "            \n",
    "        second_response = client.chat.completions.create(\n",
    "            model=TOOL_USE_MODEL,\n",
    "            messages=messages,\n",
    "        )\n",
    "        return second_response.choices[0].message.content\n",
    "    else:\n",
    "        print(\"No need to use tool...\\n\")\n",
    "        return response_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_general(query):\n",
    "    \"\"\"Use the general model to answer the query since no tool needed to answer to answer the query\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GENERAL_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Yo are a helpful assistant answer the user query.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the main function to determine if tool to use or not and if needed use to answer the user query.\n",
    "def process_query(query):\n",
    "    \"\"\"process the query and route it to the appropriate model\"\"\"\n",
    "    route = route_query(query)\n",
    "    if \"calculate\" in route:\n",
    "        response = run_with_tool(query)\n",
    "    else:\n",
    "        response = run_general(query)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"route\": route,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "\n",
      "{'query': 'what is the capital of France?', 'route': 'no tool needed to answer the query', 'response': 'The capital of France is Paris!'}\n",
      "...\n",
      "\n",
      "Tool call use available...\n",
      "\n",
      "tool call: ChatCompletionMessageToolCall(id='call_vwy1', function=Function(arguments='{\"expression\": \"25 * 4 + 10 + 90\"}', name='calculate'), type='function')\n",
      "\n",
      "model is using calculate tool\n",
      "{'query': 'calculate 25 * 4 + 10 + 90', 'route': 'calculate tool needed', 'response': 'The result of the calculation is 200.'}\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"what is the capital of France?\",\n",
    "    \"calculate 25 * 4 + 10 + 90\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"...\\n\")\n",
    "    print(process_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temperature(location: str):\n",
    "    temperatures = {\n",
    "        \"Pokhara\": 10,\n",
    "        \"Kathmandu\": 11\n",
    "    }\n",
    "    return temperatures.get(location, \"Temperature data not available\")\n",
    "\n",
    "def get_weather_condition(location: str):\n",
    "    conditions = {\n",
    "        \"Pokhara\": \"Cloudy\",\n",
    "        \"Kathmandu\": \"Sunny\"\n",
    "    }\n",
    "    return conditions.get(location, \"Weather condition data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful weather assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the weather like in Kathmandu and Pokhara?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    # tool1\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature\",\n",
    "            \"description\":\"Get the temperature of a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    # tool2\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_condition\",\n",
    "            \"description\": \"Get the weather condition of a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the location.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response: ChatCompletion(id='chatcmpl-3123a231-edc7-4cee-a929-9cc36db4440e', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_53sy', function=Function(arguments='{\"location\":\"Kathmandu\"}', name='get_weather_condition'), type='function'), ChatCompletionMessageToolCall(id='call_a5aw', function=Function(arguments='{\"location\":\"Pokhara\"}', name='get_weather_condition'), type='function')]))], created=1732812185, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_179b0f92c9', usage=CompletionUsage(completion_tokens=120, prompt_tokens=1042, total_tokens=1162, completion_time=0.1, prompt_time=0.05890477, queue_time=0.003367289999999995, total_time=0.15890477), x_groq={'id': 'req_01jdstf81he38r17k3cnt1bfq4'})\n",
      "Tool: ChatCompletionMessageToolCall(id='call_53sy', function=Function(arguments='{\"location\":\"Kathmandu\"}', name='get_weather_condition'), type='function')\n",
      "Tool: ChatCompletionMessageToolCall(id='call_a5aw', function=Function(arguments='{\"location\":\"Pokhara\"}', name='get_weather_condition'), type='function')\n",
      "It seems like I've got the results I need! According to the tools, it's currently Sunny in Kathmandu and Cloudy in Pokhara.\n"
     ]
    }
   ],
   "source": [
    "# making the call\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "print(f\"Initial response: {response}\")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "messages.append(response_message)\n",
    "\n",
    "available_functions_for_weather = {\n",
    "    \"get_temperature\": get_temperature,\n",
    "    \"get_weather_condition\": get_weather_condition\n",
    "}\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    print(f\"Tool: {tool_call}\")\n",
    "    function_name = tool_call.function.name\n",
    "    function_to_call = available_functions_for_weather.get(function_name)\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    function_response = function_to_call(\n",
    "        **function_args\n",
    "    )\n",
    "    \n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": str(function_response),\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "response_after_tool_call = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    max_tokens=3000\n",
    ")\n",
    "\n",
    "print(response_after_tool_call.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pydantic._internal._generate_schema' has no attribute 'VALIDATE_CALL_SUPPORTED_TYPES'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minstructor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgroq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Groq\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\instructor\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mode\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess_response\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m handle_response_model\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FinetuneFormat, Instructions\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, Audio\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     CitationMixin,\n\u001b[0;32m      9\u001b[0m     Maybe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     openai_moderation,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\instructor\\distil.py:99\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m         return_type \u001b[38;5;241m!=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty\n\u001b[0;32m     95\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have a return type hint that is a pydantic BaseModel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(return_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(return_type, BaseModel)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mInstructions\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;43;03m        Instructions for distillation and dispatch.\u001b[39;49;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;43;03m        :param include_code_body: Whether to include the code body in the finetuning.\u001b[39;49;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;43;03m        \"\"\"\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\instructor\\distil.py:210\u001b[0m, in \u001b[0;36mInstructions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_distil(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_distil\n\u001b[1;32m--> 210\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@validate_call\u001b[39;49m\n\u001b[0;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mtrack\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_format\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mFinetuneFormat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFinetuneFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMESSAGES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;43;03m    Track the function call and response in a log file, later used for finetuning.\u001b[39;49;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;43;03m    :param finetune_format: Format to use for finetuning. Defaults to \"raw\".\u001b[39;49;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\validate_call_decorator.py:110\u001b[0m, in \u001b[0;36mvalidate_call\u001b[1;34m(func, config, validate_return)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_call\u001b[38;5;241m.\u001b[39mupdate_wrapper_attributes(function, validate_call_wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\validate_call_decorator.py:103\u001b[0m, in \u001b[0;36mvalidate_call.<locals>.validate\u001b[1;34m(function)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(function: AnyCallableT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AnyCallableT:\n\u001b[1;32m--> 103\u001b[0m     \u001b[43m_check_function_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     validate_call_wrapper \u001b[38;5;241m=\u001b[39m _validate_call\u001b[38;5;241m.\u001b[39mValidateCallWrapper(\n\u001b[0;32m    105\u001b[0m         cast(_generate_schema\u001b[38;5;241m.\u001b[39mValidateCallSupportedTypes, function), config, validate_return, parent_namespace\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_call\u001b[38;5;241m.\u001b[39mupdate_wrapper_attributes(function, validate_call_wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\validate_call_decorator.py:25\u001b[0m, in \u001b[0;36m_check_function_type\u001b[1;34m(function)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_function_type\u001b[39m(function: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if the input function is a supported type for `validate_call`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(function, \u001b[43m_generate_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALIDATE_CALL_SUPPORTED_TYPES\u001b[49m):\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m             inspect\u001b[38;5;241m.\u001b[39msignature(cast(_generate_schema\u001b[38;5;241m.\u001b[39mValidateCallSupportedTypes, function))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydantic._internal._generate_schema' has no attribute 'VALIDATE_CALL_SUPPORTED_TYPES'"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "# Define the tool schema\n",
    "tool_schema = {\n",
    "    \"name\": \"get_weather_info\",\n",
    "    \"description\": \"Get the weather information for any location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The location for which we want to get the weather information (e.g., New York)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the Pydantic model for the tool call\n",
    "class ToolCall(BaseModel):\n",
    "    input_text: str = Field(description=\"The user's input text\")\n",
    "    tool_name: str = Field(description=\"The name of the tool to call\")\n",
    "    tool_parameters: str = Field(description=\"JSON string of tool parameters\")\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    tool_calls: list[ToolCall]\n",
    "\n",
    "# Patch Groq() with instructor\n",
    "client = instructor.from_groq(Groq(), mode=instructor.Mode.JSON)\n",
    "\n",
    "def run_conversation(user_prompt):\n",
    "    # Prepare the messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an assistant that can use tools. You have access to the following tool: {tool_schema}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Make the Groq API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        response_model=ResponseModel,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "    return response.tool_calls\n",
    "\n",
    "# Example usage\n",
    "user_prompt = \"What's the weather like in San Francisco?\"\n",
    "tool_calls = run_conversation(user_prompt)\n",
    "\n",
    "for call in tool_calls:\n",
    "    print(f\"Input: {call.input_text}\")\n",
    "    print(f\"Tool: {call.tool_name}\")\n",
    "    print(f\"Parameters: {call.tool_parameters}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
