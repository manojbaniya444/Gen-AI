{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple LLM Application with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why LangSmith\n",
    "Many of the applications built with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside the chain or agent. The best way for this is LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatModels` are instances of LangChain **Runnables**\n",
    "To simply call the model, we can pass in a list of messages to the **.invoke** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages  = [\n",
    "    SystemMessage(content=\"Translate the following from English into Nepali language.\"),\n",
    "    HumanMessage(content=\"Hello, how are you doing?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='नमस्ते, तिमी कसरी छ? (Namaste, timi kassari cha?)\\n\\n(Note: \"नमस्ते\" (Namaste) is a formal way of saying \"hello\" in Nepali, and \"तिमी कसरी छ\" (timi kassari cha?) is a way of asking \"how are you doing?\")', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 32, 'total_tokens': 113, 'completion_time': 0.064319251, 'prompt_time': 0.005561105, 'queue_time': None, 'total_time': 0.06988035599999999}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-7e5468e4-cc8e-4768-87e1-3d3019a7012a-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the **LangSmith trace** to see the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "To parse just the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते, तिमी कसरी हुनुहुन्छ? (Namaste, timi kasari hunuhunchha?)\n",
      "\n",
      "Translation:\n",
      "\n",
      "* Namaste: Hello (a traditional Nepali greeting)\n",
      "* timi: you\n",
      "* kasari: how\n",
      "* hunuhunchha: are you doing (literally \"are you fine\")\n",
      "\n",
      "So, the entire phrase \"नमस्ते, तिमी कसरी हुनुहुन्छ?\" literally means \"Hello, how are you?\"\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(messages)\n",
    "result = parser.invoke(result)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also \"chain\" the model with this output parser. This means this output parser will get called everytime in this chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नेपाल एशिया महादेशमा सुन्दर देश हो।'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages2 = [\n",
    "    SystemMessage(content=\"Translate the following from English into Nepali language. Just response with the translation and nothing else.\"),\n",
    "    HumanMessage(content=\"Nepal is a beautiful country located in Asia continent.\")\n",
    "]\n",
    "\n",
    "chain.invoke(messages2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "What I did above was abstraction over prompt template. Usually **messages** is constructed from a combination of user input and application logic.This application logic takes the raw user  input and transforms it into a list of messages ready to pass to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language} language and respond with the translation just respond with the translation and nothing else.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a **Prompt Template** will be a combination of the **System Template** as well as a simpler template for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into Nepali:'), HumanMessage(content='Discipline is the bridge between goals and accomplishment.')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"Nepali\",\n",
    "        \"text\": \"Discipline is the bridge between goals and accomplishment.\"\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Nepali:'),\n",
       " HumanMessage(content='Discipline is the bridge between goals and accomplishment.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining together components with LCEL\n",
    "we can now combine this with the model and output parser from above using the `pipe (|)` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "संस्कार हुन्छ काम आउने र पूरा हुने बीच को पुल हो ।\n"
     ]
    }
   ],
   "source": [
    "custom_messages = {\n",
    "    \"language\": \"Nepali\",\n",
    "    \"text\": \"Discipline is the bridge between goals and accomplishment.\"\n",
    "}\n",
    "\n",
    "response = chain.invoke(custom_messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this can be easilt be served using `**LangServe**`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
