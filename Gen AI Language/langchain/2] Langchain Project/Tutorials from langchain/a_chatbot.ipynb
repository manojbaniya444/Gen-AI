{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatModels are the instances of LangChain `Runnables`, they expose a standard interface for interacting with them.\n",
    "We can simple use `invoke` method to call the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Manoj! Nice to meet you! Is there something I can help you with or would you like to chat?', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 15, 'total_tokens': 40, 'completion_time': 0.019154745, 'prompt_time': 0.00309007, 'queue_time': None, 'total_time': 0.022244815}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-da85ea29-5c07-45f9-a6b1-dedc524f5ce9-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi I am Manoj\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I apologize, but I don't know your name. I'm a large language model, I don't have the ability to know or retain personal information about individuals. Each time you interact with me, it's a new conversation and I don't have any prior knowledge or context about you. If you'd like to share your name with me, I'd be happy to learn it!\", response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 15, 'total_tokens': 93, 'completion_time': 0.061547363, 'prompt_time': 0.003444526, 'queue_time': None, 'total_time': 0.064991889}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_f128cfcd6c', 'finish_reason': 'stop', 'logprobs': None}, id='run-36b7bc5a-1b9c-4ed1-bd08-d775cebefe03-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([\n",
    "    HumanMessage(content=\"What is my name?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it doesnt take the previous conversation turn into context and cannot answer the qestion. This is not how a chatbot should work.\n",
    "\n",
    "To get around this we need to pass the entire conversation history into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Manoj!', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 41, 'total_tokens': 48, 'completion_time': 0.004798075, 'prompt_time': 0.006930433, 'queue_time': None, 'total_time': 0.011728507999999999}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-37943557-c7d9-44c8-9211-ee3edd6b55a2-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi Iam Manoj\"),\n",
    "    AIMessage(content=\"Hello Manoj! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What is my name?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a good response.\n",
    "This is the basic underpinning a chatbot's ability to interact conversationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and can make it stateful which will keep track of inputs and outputs of the model and store them in some datastore.\n",
    "\n",
    "Any future interactions will then load those messages and pass them into the chain as part of the input.\n",
    "\n",
    "**Langchain_community** must be installed first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "# session_id: This session_id is used to distinguish between separate conversaitons,\n",
    "\n",
    "# will return a Message History object\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "        \n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    model,\n",
    "    get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create a config that we pass into the runnable every time.\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"111\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 7db3aebf-5f05-405a-b947-5e8069cb1f59 not found for run 4cb79d73-e927-49d8-ae5b-5a78647528a9. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Manoj! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi I am Manoj\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 4b852790-7bfe-4a7c-a224-a16b3b66208f not found for run 4dd2ab63-1c94-4751-807b-37c649907a48. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Manoj!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now again running with the same config which contain the same session id so that we are in the same conversation,\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e6ac75bc-8f1b-48f8-baa8-0d4c020c1091 not found for run 712d9c53-bc17-41af-b669-f7b20bb3d8bf. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I apologize, but I'm a large language model, I don't have the ability to know your name or any personal information about you. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. If you'd like to share your name with me, I'd be happy to know it and address you by name!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we change the session id then it will be a new conversation,\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey what is my name?\")],\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": \"222\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'111': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi I am Manoj'), AIMessage(content=\"Hi Manoj! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 15, 'total_tokens': 42, 'completion_time': 0.020707135, 'prompt_time': 0.003414331, 'queue_time': None, 'total_time': 0.024121466}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4cb79d73-e927-49d8-ae5b-5a78647528a9-0'), HumanMessage(content='What is my name?'), AIMessage(content='Your name is Manoj!', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 56, 'total_tokens': 63, 'completion_time': 0.004812692, 'prompt_time': 0.00917908, 'queue_time': None, 'total_time': 0.013991772}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_f128cfcd6c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4dd2ab63-1c94-4751-807b-37c649907a48-0')]),\n",
       " '222': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hey what is my name?'), AIMessage(content=\"I apologize, but I'm a large language model, I don't have the ability to know your name or any personal information about you. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. If you'd like to share your name with me, I'd be happy to know it and address you by name!\", response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 16, 'total_tokens': 92, 'completion_time': 0.060852438, 'prompt_time': 0.004008409, 'queue_time': None, 'total_time': 0.064860847}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-712d9c53-bc17-41af-b669-f7b20bb3d8bf-0')])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi I am Manoj'),\n",
       " AIMessage(content=\"Hi Manoj! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 15, 'total_tokens': 42, 'completion_time': 0.020707135, 'prompt_time': 0.003414331, 'queue_time': None, 'total_time': 0.024121466}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4cb79d73-e927-49d8-ae5b-5a78647528a9-0'),\n",
       " HumanMessage(content='What is my name?'),\n",
       " AIMessage(content='Your name is Manoj!', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 56, 'total_tokens': 63, 'completion_time': 0.004812692, 'prompt_time': 0.00917908, 'queue_time': None, 'total_time': 0.013991772}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_f128cfcd6c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4dd2ab63-1c94-4751-807b-37c649907a48-0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"111\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can make more complicated and personalized chatbot by adding in a prompt template.\n",
    "\n",
    "**Prompt Templates**\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with.(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        # messagesplaceholder to pass all the messages in a dictionary with a \"messages\" key where that contains a list of messages.\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Manoj Baniya! I'm happy to be your helpful assistant. How can I assist you today? Do you have a specific question, topic you'd like to discuss, or task you'd like help with? I'm all ears!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hello  I am Manoj Baniya.\")]\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"new_session\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 5e8d7aca-cc1f-499a-9b5d-4355bb1aba70 not found for run c3b00dfd-7fa6-4a28-8b58-2939afb916c4. Treating as a root run.\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hello I am Manoj Baniya.\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Manoj Baniya! How can I assist you today? Do you have a specific question, topic you'd like to discuss, or perhaps a task you'd like help with? I'm all ears!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit more complicated Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 1: for the system\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language} language.\"\n",
    "        ),\n",
    "        # 2: for the user\n",
    "        # All the history messages will be here in the \"messages\" in dict format which will contain a list of messages.\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Namaste Manoj Baniya ji! Welcome, I'm glad to meet you. You're from Nepal, right? How can I help you today? Kya problem hai?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **language** is a variable that should be passed in the messages when calling\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hello my name is Manoj Baniya, I am from Nepal.\")\n",
    "        ],\n",
    "        \"language\": \"Hinglish\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aapka naam kya hai? Sorry, I didn't understand that. You didn't tell me your name, so I can't tell you what it is. Can you please introduce yourself?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hello what is my name?\")\n",
    "        ],\n",
    "        \"language\": \"Hinglish\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally wrapping this new prompt template with input variable in the MessageHistory to keep memory in the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, # which is the prompt | model\n",
    "    get_session_history, # function to get the session history\n",
    "    input_messages_key=\"messages\", # key to pass the messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"333\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 6bb890a0-0705-457d-bae5-e8fdb050329a not found for run 1c9826d7-b305-4bc9-8cf8-2facf43bbc08. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Namaste Manoj Baniya ji! Aapka swagat hai! (Hello Manoj Baniya ji! Welcome!) Nepal se aap hai? (Are you from Nepal?) It's great to have you here! How can I assist you today?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hello my name is Manoj Baniya and I am from Nepal.\")\n",
    "        ],\n",
    "        \"language\": \"Hinglish\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 551291db-bcea-4b87-8ff6-02e7a5fec2d1 not found for run 797961b7-add1-4dab-88b6-2f5fddd47dc3. Treating as a root run.\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hey who am I?\")\n",
    "        ],\n",
    "        \"language\": \"Hinglish\"\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aap Manoj Baniya hai, Nepal se aapka naam hai! (You are Manoj Baniya, and your name is from Nepal!)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important!\n",
    "\n",
    "One important to understand well while building chatbots is to manage conversation history.\n",
    "\n",
    "If left unmanaged the list of messages will grow `unbounded and potentially cause overflow the context window of the LLM`\n",
    "\n",
    "So it is important to add a step that limits the size of the messages we are passing in.We do this BEFORE the prompt template but AFTER we load previous messages from Message History\n",
    "\n",
    "We can do this by adding a simple step in front of the prompt that modifies the `messages` key appropriately, and then wrap that new chain in the Message History class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain **trim_messages** helper function is available to reduce how many messages we are sending to the model.\n",
    "The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\acer\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant answer the following questions to the best of your ability\"),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages, SystemMessage\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant answer the following questions to the best of your ability\"),\n",
    "    HumanMessage(content=\"hi! I'm Manoj Baniya and I am from Nepal.\"),\n",
    "    AIMessage(content=\"hi! Mano I am assistant. How can I help you today?\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know your name, buddy!\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# To use trimmer in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"Hello what is my name?\")],\n",
    "        \"language\": \"Hinglish\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maine pata hai! You like vanilla ice cream!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but if we ask other quesions it can answer it\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"which ice cream do i like?\")],\n",
    "        \"language\": \"Hinglish\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping this in Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"999\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run afd5c7da-9701-421e-9dc0-ed3e5ad24ee2 not found for run 2fedfb9f-3ffc-4347-a66f-4da435e24d27. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know, sorry!\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is my name?\")],\n",
    "        \"language\": \"Hinglish\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the first message where we stated our name has been trimmed. Plus there's now two new messages in the chat history (our latest question and the latest response). This means that even more information that used to be accessible in our conversation history is no longer available! In this case our initial math question has been trimmed from the history as well, so the model no longer knows about it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LamgSmith trace to know what is happening under the hood here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "This is to improve the User Experience with chatbot where each generated token is shown as it is generating so user dont have to wait long till all the tokens are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e4f5a96a-4636-4e9f-a843-63fcc7db2b82 not found for run 0cb33fb9-cc1e-4ccb-9a8b-b01bb5cb9c79. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Man|oj|!| Nice| to| meet| you|!\n",
      "\n",
      "|Here|'s| a| joke| for| you|:\n",
      "\n",
      "|Why| couldn|'t| the| bicycle| stand| up| by| itself|?\n",
      "\n",
      "|(W|ait| for| it|...)\n",
      "\n",
      "|Because| it| was| two|-t|ired|!\n",
      "\n",
      "|Hope| that| made| you| smile|!| Do| you| want| to| hear| another| one|?||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm Manoj. tell me a nice joke\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
