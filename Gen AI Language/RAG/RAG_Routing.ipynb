{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "llm will be choosing which path will it take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Generative AI\\\\Gen AI Language\\\\local LLM\\\\local embedding\\\\sentence-transformers-local\\\\all-MiniLM-L6-v2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "embedding_path = os.getenv(\"EMBEDDING_PATH\")\n",
    "embedding_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "emmbedding_model = SentenceTransformer(embedding_path)\n",
    "emmbedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002565766EFC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002565752ADE0>, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq()\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical and Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    '''Route a user query to the most relevant source.'''\n",
    "    datasource: Literal[\"youtube\", \"linkedin\", \"twitter\"] = Field(\n",
    "        ..., description=\"Given a user question choose which social media platform to query.\"\n",
    "    )\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = '''You are an expert at routing a user question to the appropriate social media platform.'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at routing a user question to the appropriate social media platform.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002565766EFC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002565752ADE0>, model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant source.', 'parameters': {'type': 'object', 'properties': {'datasource': {'description': 'Given a user question choose which social media platform to query.', 'enum': ['youtube', 'linkedin', 'twitter'], 'type': 'string'}}, 'required': ['datasource']}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'RouteQuery'}}}, config={}, config_factories=[])\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='youtube')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '''\n",
    "I am looking for a tutorial on how to code.\n",
    "'''\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youtube'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"youtube\" in result.datasource:\n",
    "        return \"https://www.youtube.com\"\n",
    "    elif \"linkedin\" in result.datasource:\n",
    "        return \"https://www.linkedin.com\"\n",
    "    else:\n",
    "        return \"https://ww.x.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_route(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_route(router.invoke({\"question\": \"I am looking for a job.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ww.x.com'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "full_chain.invoke({\"question\": \"I want to get the latest news on the stock market.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "physics_template = '''\n",
    "You are a very smart physics teacher. \\\n",
    "You are great at answering questions about physics in a concise and clear easy way \\\n",
    "When you do not know the answer to a question you admit you do not know the answer.\n",
    "\n",
    "Here is the question:\n",
    "{query} \n",
    "'''\n",
    "\n",
    "math_template = '''\n",
    "You are a very good math teacher. You are great at answering math question because you able to break down hard problems into their component parts.\n",
    "\n",
    "Answer the component parts and then put them together to answer the question.\n",
    "\n",
    "Here is the question: {query}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [physics_template, math_template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(documents: list[str]):\n",
    "    embedded_documents = []\n",
    "    for document in documents:\n",
    "        encoded = emmbedding_model.encode(document)\n",
    "        embedded_documents.append(encoded)\n",
    "        \n",
    "    return embedded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings = embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = embed_documents([\"hello world\"])\n",
    "test_embedding_another = emmbedding_model.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_similarity([test_embedding_another], prompt_embeddings)\n",
    "sim_another = cosine_similarity(test_embedding, prompt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.8081372]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [emmbedding_model.encode(\"I love chocolate\")],\n",
    "    embed_documents([\"I love chocolate\", \"I hate chocolate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07782761 0.04691144]]\n",
      "[[0.07782761 0.04691144]]\n"
     ]
    }
   ],
   "source": [
    "print(sim)\n",
    "print(sim_another)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07782761, 0.04691144]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route question\n",
    "def prompt_router(input):\n",
    "    # embed query\n",
    "    query_embedding = embed_documents([input[\"query\"]])\n",
    "    prompt_embeddings = embed_documents(prompt_templates)\n",
    "    \n",
    "    similarity = cosine_similarity(query_embedding, prompt_embeddings)\n",
    "    most_similar_index = similarity[0].argmax()\n",
    "    most_similar = prompt_templates[most_similar_index]\n",
    "    #\n",
    "    print(\"Using MATH: \" if most_similar == math_template else \"Using PHYSICS: \")\n",
    "    prompt = PromptTemplate.from_template(most_similar)\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  query: RunnablePassthrough()\n",
       "}\n",
       "| RunnableLambda(prompt_router)\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002565766EFC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002565752ADE0>, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm\n",
    "    | (lambda x : x.content)\n",
    ")\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS: \n",
      "The speed of light is approximately 299,792 kilometers per second, or about 186,282 miles per second. This is a fundamental constant in physics and is denoted by the letter c. It is the maximum speed at which information or matter can travel.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is the speed of light?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS: \n",
      "Pythagorean theorem states that in a right-angled triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the lengths of the other two sides. This can be written as: a² + b² = c², where c represents the length of the hypotenuse, and a and b represent the lengths of the other two sides.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is pythagoras theorem?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS: \n",
      "Force is a push or pull on an object resulting from the object's interaction with another object. It is a vector quantity, meaning it has both magnitude (size) and direction. Forces can cause an object to move, stop moving, or change direction. The unit of force in the International System of Units (SI) is the newton (N).\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is force\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force is a push or pull upon an object resulting from the object's interaction with another object. It is a vector quantity, meaning that it has both magnitude (size) and direction. Forces can cause an object to move, stop moving, or change its direction. They can also cause deformation or damage to an object. Forces can arise from various sources, such as gravity, friction, normal force, tension, and air resistance. The unit of force in the International System of Units (SI) is the newton (N), which is defined as the force required to accelerate a mass of one kilogram at a rate of one meter per second squared.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what is force\").content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
