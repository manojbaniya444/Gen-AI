{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a language model, I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello how are you?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 22 divided by 7?\")\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='22 divided by 7 is equal to 3.14 (rounded to two decimal places).', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 29, 'total_tokens': 49, 'completion_time': 0.015164334, 'prompt_time': 0.005118398, 'queue_time': None, 'total_time': 0.020282731999999998}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-95b6191c-9a65-4caf-aea5-757c9cddf094-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 divided by 7 is equal to 3.14 (rounded to two decimal places).\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 22 divided by 7?\"),\n",
    "    AIMessage(content=\"22 divided by 7 is 3.142857142857143\"),\n",
    "    HumanMessage(content=\"What is 10 times 10?\"),\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 times 10 is 100.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "# model = ChatOpenAI(model=\"gpt-40\")\n",
    "# model - ChatAnthropic(model=\"claude3)\n",
    "\n",
    "# many more llms model available but similar to the above in use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time conversation with user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! It's nice to meet you! Is there something I can help you with or would you like to chat?\n",
      "AI: I'm doing well, thank you for asking! I'm a large language model, so I don't have emotions or feelings like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going?\n",
      "AI: I'm an AI assistant, which means I'm a computer program designed to help and communicate with humans. I'm a type of language model, which means I can understand and respond to natural language inputs, like sentences and questions.\n",
      "\n",
      "I can help with a wide range of tasks, such as:\n",
      "\n",
      "* Answering questions on various topics\n",
      "* Providing definitions and explanations\n",
      "* Generating text or summaries\n",
      "* Offering suggestions and recommendations\n",
      "* Even having a conversation or chatting with you!\n",
      "\n",
      "I'm constantly learning and improving my abilities, so the more I interact with users like you, the better I become. I'm here to help and provide information, so feel free to ask me anything!\n",
      "CHAT HISTORY\n",
      "[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='hello'), AIMessage(content=\"Hello! It's nice to meet you! Is there something I can help you with or would you like to chat?\"), HumanMessage(content='how are you'), AIMessage(content=\"I'm doing well, thank you for asking! I'm a large language model, so I don't have emotions or feelings like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going?\"), HumanMessage(content='who are you?'), AIMessage(content=\"I'm an AI assistant, which means I'm a computer program designed to help and communicate with humans. I'm a type of language model, which means I can understand and respond to natural language inputs, like sentences and questions.\\n\\nI can help with a wide range of tasks, such as:\\n\\n* Answering questions on various topics\\n* Providing definitions and explanations\\n* Generating text or summaries\\n* Offering suggestions and recommendations\\n* Even having a conversation or chatting with you!\\n\\nI'm constantly learning and improving my abilities, so the more I interact with users like you, the better I become. I'm here to help and provide information, so feel free to ask me anything!\")]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "# setting an initial message\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    # add user message to chat history\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    \n",
    "    # get ai response with history\n",
    "    result = llm.invoke(chat_history)\n",
    "    response = result.content\n",
    "    # again appending the ai message to the chat history\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    \n",
    "    print(f\"AI: {response}\")\n",
    "    \n",
    "print(\"CHAT HISTORY\")\n",
    "print(chat_history)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
