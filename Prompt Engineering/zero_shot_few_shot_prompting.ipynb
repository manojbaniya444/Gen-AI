{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Prompting\n",
    "\n",
    "It means that the prompt used to interact with the model wont contain examples or demonstrations.\n",
    "\n",
    "The zero shot prompt directly instructs the model to perform a task without any additional examples to steer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-6377f408-4a2a-4c47-87ea-4c3c4dec9a02', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sentiment of the text is: Positive.', role='assistant', function_call=None, tool_calls=None))], created=1730313367, model='llama3-70b-8192', object='chat.completion', system_fingerprint='fp_753a4aecf6', usage=CompletionUsage(completion_tokens=10, prompt_tokens=42, total_tokens=52, completion_time=0.028571429, prompt_time=0.004644054, queue_time=0.009952746, total_time=0.033215483), x_groq={'id': 'req_01jbfbdbyefpj8e3tvy6q2gad2'})\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "            classify the text into neutral, negative or positive.\n",
    "            \n",
    "            Text: \"I love this product, it's amazing!\"\n",
    "            \n",
    "            Sentiment:\n",
    "            \"\"\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the text is: Positive.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lionel Messi is from Rosario, Argentina. He was born on June 24, 1987, in Rosario, Argentina, to Jorge Messi and Celia Cuccittini. Messi grew up in a family of Italian and Spanish immigrants and began playing football at a young age in his hometown before joining Newell's Old Boys and eventually moving to Spain to join FC Barcelona's youth academy.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"where is Lionel Messi from?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompting\n",
    "For more complex task zero shot prompting fail. Few shot prompting technique can be used as a technique to enable in context learning where we provide demonstrations in the prompt to steer the model for better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example sentence:\n",
      "\n",
      "\"During the harvest season, our entire village participates in parma to ensure everyone's crops are planted and harvested on time.\"\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "In Nepal \"Parma\" is a reciprocal exchange of labout where people come together to conduct similar work for each other during agricultural season.\n",
    "\n",
    "An example text using Parma is:\n",
    "'I am going for parma to my neighbor's house'\n",
    "\n",
    "An example of a sentence that use parma is:\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are expert in making sentence using any text.\"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": few_shot_prompt\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.2,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grumblegrumble'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "John is happy! // hahaha\n",
    "Harry is sad. // huhuhu\n",
    "Alex is surprised. // wowww\n",
    "\n",
    "Jane is angry. // \n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's calculate the sum of each group:\n",
      "\n",
      "1. 10, 20, 30, 40, 50: 10 + 20 + 30 + 40 + 50 = 150 (even)\n",
      "A: True\n",
      "\n",
      "2. 11, 22, 13, 11, 10: 11 + 22 + 13 + 11 + 10 = 77 (odd)\n",
      "A: False\n",
      "\n",
      "3. 9, 1, 2, 13, 3: 9 + 1 + 2 + 13 + 3 = 28 (even)\n",
      "A: True\n",
      "\n",
      "4. 12, 2, 11, 32, 1: 12 + 2 + 11 + 32 + 1 = 58 (even)\n",
      "A: True\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "The sum in this group is even: 10, 20, 30, 40, 50\n",
    "A: True\n",
    "The sum in this group is even: 11, 22, 13, 11, 10\n",
    "A: False\n",
    "The sum in this group is even: 9, 1, 2, 13, 3\n",
    "A: True\n",
    "The sum in this group is even: 12, 2, 11, 32, 1\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        },\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classic Caesar cipher-style substitution!\n",
      "\n",
      "Let's analyze the patterns:\n",
      "\n",
      "* 'a' maps to 'c'\n",
      "* 'b' maps to 'd'\n",
      "* 'c' maps to 'e'\n",
      "* 'd' maps to 'f'\n",
      "* 'e' maps to 'g'\n",
      "* 'f' maps to 'h'\n",
      "* ...and so on.\n",
      "\n",
      "Now, let's apply this substitution to 'secret':\n",
      "\n",
      "* 's' maps to 't' (since 's' is the 19th letter of the alphabet and 't' is the 20th)\n",
      "* 'e' maps to 'g'\n",
      "* 'c' maps to 'e'\n",
      "* 'r' maps to 't'\n",
      "* 'e' maps to 'g'\n",
      "* 't' maps to 'v'\n",
      "\n",
      "So, the hash for 'secret' is 'teghtv'.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The hash for the word 'aaa' is 'ccc', The has for the word 'abc' is 'cde', The hash for the word 'efg' is 'hij'. The hash for the word 'rst' is 'tuv' What is the hash for 'secret'?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It looks like there's a pattern in the way letters are mapped. Let’s analyze how each word's letters are transformed into their hash to find the rule.\n",
      "\n",
      "For each of the given words and their hashes:\n",
      "\n",
      "\"aaa\" → \"ccc\": Each letter seems to be shifted forward by 2 in the alphabet.\n",
      "\"abc\" → \"cde\": Each letter is shifted forward by 2.\n",
      "\"efg\" → \"hij\": Each letter is again shifted forward by 2.\n",
      "\"rst\" → \"tuv\": Each letter is shifted forward by 2.\n",
      "It seems that the \"hash\" operation shifts each character in the word forward by 2 letters in the alphabet.\n",
      "\n",
      "To find the hash for \"secret,\" let's apply the same rule:\n",
      "\n",
      "'s' shifted by 2 becomes 'u'\n",
      "'e' shifted by 2 becomes 'g'\n",
      "'c' shifted by 2 becomes 'e'\n",
      "'r' shifted by 2 becomes 't'\n",
      "'e' shifted by 2 becomes 'g'\n",
      "'t' shifted by 2 becomes 'v'\n",
      "So, the hash for \"secret\" would be \"ugetgv\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_response = '''\n",
    "It looks like there's a pattern in the way letters are mapped. Let’s analyze how each word's letters are transformed into their hash to find the rule.\n",
    "\n",
    "For each of the given words and their hashes:\n",
    "\n",
    "\"aaa\" → \"ccc\": Each letter seems to be shifted forward by 2 in the alphabet.\n",
    "\"abc\" → \"cde\": Each letter is shifted forward by 2.\n",
    "\"efg\" → \"hij\": Each letter is again shifted forward by 2.\n",
    "\"rst\" → \"tuv\": Each letter is shifted forward by 2.\n",
    "It seems that the \"hash\" operation shifts each character in the word forward by 2 letters in the alphabet.\n",
    "\n",
    "To find the hash for \"secret,\" let's apply the same rule:\n",
    "\n",
    "'s' shifted by 2 becomes 'u'\n",
    "'e' shifted by 2 becomes 'g'\n",
    "'c' shifted by 2 becomes 'e'\n",
    "'r' shifted by 2 becomes 't'\n",
    "'e' shifted by 2 becomes 'g'\n",
    "'t' shifted by 2 becomes 'v'\n",
    "So, the hash for \"secret\" would be \"ugetgv\".\n",
    "'''\n",
    "print(gpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot prompting only is not enough to get reliable response. Other prompting technique can be applied to get correct response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
